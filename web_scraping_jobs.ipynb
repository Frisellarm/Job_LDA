{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Web-Scraping Using Selenium\n",
    "Note: This is intented for practice to extract real time job descriptions. Please adhere to the Robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you are using Selenium for the first time, please download the webdriver and note the filepath\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Used to allow the browser to load. \n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#used as a import for dateing the CSV\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is an input for jobs and location\n",
    "occupation = str(input(\"Job title: \"))\n",
    "loc = str(input(\"City, State(i.e. TX): \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will need to folder that the webdriver is in:\n",
    "\n",
    "Download the webdriver here: https://www.seleniumhq.org/download/\n",
    "\n",
    "Note the path you download the file from. Locate the file and unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_chromedriver = \"./chromedriver_win32/chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup browser window\n",
    "browser = webdriver.Chrome(executable_path= path_to_chromedriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds the data in URL format by removing any white space and adding the data. \n",
    "def Keyword_to_url(kw):\n",
    "    kw = kw.strip(\" \").replace(\" \",\"%20\")\n",
    "    return kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be used as the URL input for multiple job searches. \n",
    "radius = str(\"5\")\n",
    "page_size = \"100\" #how many pages would you like to get. \n",
    "source = 'IN' #stands for indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give it the siteD\n",
    "url = 'https://www.careeronestop.org/Toolkit/Jobs/find-jobs.aspx?keyword='+ \\\n",
    "    Keyword_to_url(occupation)+\\\n",
    "    '&ajax=0&location='+loc+'&radius='+radius+ \\\n",
    "    '&source='+ source +'&pagesize='+page_size+'&sortcolumns=accquisitiondate&sortdirections=DSC'\n",
    "browser.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_num = len(browser.find_elements_by_xpath('//*[@data-title=\"Job Title\"]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_links = browser.find_elements_by_xpath('//*[@data-title=\"Job Title\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these catch the title of the job details. \n",
    "company = []\n",
    "position = []\n",
    "location = []\n",
    "date_posted = []\n",
    "job_url = []\n",
    "pos_description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_job_listings(num):\n",
    "    \n",
    "    \"\"\"\n",
    "    Works on an individual link basis.\n",
    "    Creates dataframe from the link and job listed on the site. \n",
    "    the job descriptions will be added later.\n",
    "    \"\"\"\n",
    "    \n",
    "    num = int(num)\n",
    "    position.append(job_links[num].text)\n",
    "    company.append(browser.find_elements_by_xpath('//*[@data-title=\"Company\"]')[num].text)\n",
    "    location.append(browser.find_elements_by_xpath('//*[@data-title=\"Location\"]')[num].text)\n",
    "    date_posted.append(browser.find_elements_by_xpath('//*[@data-title=\"Date Posted\"]')[num].text)\n",
    "    job_url.append(job_links[num].find_element_by_tag_name(\"a\").get_attribute('href'))\n",
    "    \n",
    "    return pd.DataFrame({'company': company , \n",
    "                     'position': position, \n",
    "                     'location':location,\n",
    "                     'date_posted':date_posted,'job_url': job_url,\n",
    "                     'position_description':'0'},columns=['company','location','position','date_posted','job_url','position_description'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop to obtain all information for each job in the table.\n",
    "for links in range(listing_num):\n",
    "    collect_job_listings(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe of all data collected.\n",
    "job_list = pd.DataFrame({'company': company , \n",
    "                     'position': position, \n",
    "                     'location':location,\n",
    "                     'date_posted':date_posted,'job_url': job_url,\n",
    "                     'position_description':'0'},columns=['company','location','position','date_posted','job_url','position_description']).drop_duplicates()\n",
    "#drops all duplicates and replaces them with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks for page errors and to see which ones. \n",
    "err = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [08:18<00:00,  4.98s/it]\n"
     ]
    }
   ],
   "source": [
    "#Loops the urls and receives the content from the webpage.\n",
    "for i in tqdm(range(job_list.shape[0])):\n",
    "    time.sleep(np.random.randint(2,7))\n",
    "    try:\n",
    "        browser.get(job_list['job_url'][i])\n",
    "        pos_des = browser.find_element_by_class_name(\"snip\").text\n",
    "        pos_des = pos_des.replace(\"\\n\",' ')\n",
    "        job_list['position_description'][i] = job_list['position_description'][i].replace('0',pos_des)\n",
    "    except:\n",
    "        err.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(job_list['job_url'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list.to_csv(\"./data/\"+str(datetime.datetime.today())[:10]+\"_\"+occupation+\"_\"+loc+\" job_list.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
